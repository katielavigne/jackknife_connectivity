{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea54a450",
   "metadata": {},
   "source": [
    "## Jackknife Bias Estimation for Morphometric Connectivity\n",
    "\n",
    "This notebook describes the steps we used for calculating structural covariance and graph measures from CIVET-based cortical thickness:\n",
    "\n",
    "1. Install & Import Packages\n",
    "2. Read CIVET Files\n",
    "3. DKT Parcellation\n",
    "4. Covariate Regression\n",
    "5. Structural Covariance\n",
    "6. Jackknife Bias Estimation\n",
    "7. Graph Theory\n",
    "\n",
    "MATLAB-Code available [here](https://github.com/katielavigne/civetsurf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a10e38d",
   "metadata": {},
   "source": [
    "### 1. Data Preparation\n",
    "\n",
    "#### 1a. Install & Import Packages \n",
    "\n",
    "This step ensures all needed packages are either installed or, if already installed, imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f42e0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install packages needed for all steps of the analyses\n",
    "!pip install bctpy\n",
    "!pip install statsmodels\n",
    "!pip install pandas \n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4575772c-ff2a-4962-9954-8658663f9e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os, glob\n",
    "import pandas as pd\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25870b1f-8e88-4ca9-8b01-1774de86372c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bct\n",
    "import statsmodels.formula.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6b8e67-ecaa-4aa7-8ca7-9711e335b6f8",
   "metadata": {},
   "source": [
    "#### 1b. Define Paths & Files\n",
    "\n",
    "Modify this step for your specific data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f455b8b-3f0b-4c74-841c-74f339cea3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define paths, files & variables\n",
    "civpath = '/scratch/katie/ukbb/data/civet_0mm/' # location containing civet outputs\n",
    "dpath = '/scratch/katie/ukbb/data/' # location to load glimfile & save outputs\n",
    "\n",
    "gfile = 'ukbb_glimfile_final.csv' # glimfile\n",
    "dktvert_file = 'CIVET_2.0_DKT.txt' # DKT CIVET-based vertex file\n",
    "dktinfo_file = 'DKT.csv' # DKT parcellation info\n",
    "\n",
    "measure = 'thickness' # structural metric (e.g., thickness, surface_area, volume)\n",
    "idvar = 'eid' # unique subject ID variable\n",
    "group = 'dx' # grouping variable (if applicable)\n",
    "covars = ['age','mean_' + measure] # covariates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9652d30",
   "metadata": {},
   "source": [
    "### 2. Read Files\n",
    "\n",
    "This step reads [CIVET](https://www.bic.mni.mcgill.ca/ServicesSoftware/CIVET-2-1-0-Table-of-Contents)-based cortical surface text files from a specified directory. CIVET outputs text files consisting of  values for a given structural metric (cortical thickness, surface area, volume) for each vertex separately for each hemisphere. Using cortical thickness as an example with data from the [UK BioBank](https://www.ukbiobank.ac.uk/), this script loads the files for both hemispheres and merges them into a dataframe displaying all vertices. Further, mean cortical metric values are calculated for each participant by taking the mean of all vertices per participant, and a total metric value is calculated by summing the values of each participant. Depending on the cortical metric, one would use either the mean (thickness) or total (surface area) values for later steps.\n",
    "\n",
    "Inputs are:\n",
    "- cortical thickness files for each hemisphere of N participants\n",
    "\n",
    "Outputs are:\n",
    "- a dataframe with the dimensions N x 81,924 vertices\n",
    "- a dataframe with mean thickness values\n",
    "- a dataframe with total thickness values \n",
    "\n",
    "___\n",
    "\n",
    "For high performance computing, see:\n",
    "- readfiles.py\n",
    "- readfiles.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045ca9d9-702c-4b81-9601-ddec4327295f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find files & IDs\n",
    "Lfiles = glob.glob(civpath + '*left*')\n",
    "Lfiles.sort()\n",
    "Rfiles = glob.glob(civpath + '*right*')\n",
    "Rfiles.sort()\n",
    "subjIDs = [ids.split('/')[-1].split('_')[1] for ids in Lfiles] # modify this for different filename conventions\n",
    "\n",
    "# make dataframe & save as pickle\n",
    "Ldf = pd.concat((pd.read_csv(Lf, dtype=float, header=None).T for Lf in Lfiles))\n",
    "Rdf = pd.concat((pd.read_csv(Rf, dtype=float, header=None).T for Rf in Rfiles))\n",
    "df = pd.concat([Ldf,Rdf], axis=1)\n",
    "df.index = subjIDs\n",
    "df.index.names = [idvar]\n",
    "df.to_pickle(dpath + measure + '_vertexdata.pkl')\n",
    "\n",
    "# calculate and save mean anatomical measure (mean thickness)\n",
    "mean_measure = pd.DataFrame(pd.to_numeric(df.mean(axis=1)), columns=['mean_' + measure])\n",
    "mean_measure.to_csv(dpath + 'mean_' + measure + '.csv')\n",
    "\n",
    "# calculate and save total anatomical measure (total thickness)\n",
    "tot_measure = pd.DataFrame(pd.to_numeric(df.sum(axis=1)), columns=['total_' + measure])\n",
    "tot_measure.to_csv(dpath + 'total_' + measure + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fa51e5",
   "metadata": {},
   "source": [
    "### 3. DKT-Parcellation \n",
    "\n",
    "This step parcellates the data of the 81,924 vertices into 62 DKT-regions ([Klein & Tourville, 2012](https://doi.org/10.3389/fnins.2012.00171)).\n",
    "\n",
    "Inputs are:\n",
    "- the previously created dataframe with all 81,924 vertices \n",
    "- a DKT-file indicating the DKT regions (see the separate file \"DKT.csv\" in the repository; from [MATLAB code](https://github.com/katielavigne/civetsurf) and the [CIVET Manual](https://www.bic.mni.mcgill.ca/ServicesSoftware/CIVET-2-1-0-Table-of-Contents))\n",
    "- a DKT-file indicating which vertex belongs to which DKT-region (see the separate file \"CIVET_2.0_DKT.txt\" in the repository; from [MATLAB code](https://github.com/katielavigne/civetsurf) and the [CIVET Manual](https://www.bic.mni.mcgill.ca/ServicesSoftware/CIVET-2-1-0-Table-of-Contents))\n",
    "\n",
    "Outputs are:\n",
    "- a dataframe with the parcellated data of the dimensions N x 62 \n",
    "\n",
    "___\n",
    "\n",
    "For high performance computing see:\n",
    "- parcellation.py\n",
    "- parcellation.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30375bb6-dca5-4261-941a-5cf1dcab2612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read DKT and data files\n",
    "#df = pd.read_pickle(dpath + measure + '_vertexdata.pkl')\n",
    "dktvert = pd.read_csv(dktvert_file, dtype=str, names=['roi'])\n",
    "dktvert\n",
    "\n",
    "dktinfo = pd.read_csv(dktinfo_file, dtype=str)\n",
    "\n",
    "# parcellation\n",
    "parc = pd.DataFrame(index= df.index.copy())\n",
    "for r in range(len(dktinfo)):\n",
    "    roi = dktinfo.label_number[r]\n",
    "    abr = dktinfo.abbreviation[r]\n",
    "    means = pd.DataFrame(df.iloc[:,dktvert.index[dktvert.roi == roi]].mean(axis=1),columns=[abr], index= df.index.copy())\n",
    "    parc = pd.concat([parc,means], axis = 1)\n",
    "\n",
    "# write to csv\n",
    "parc.to_csv(dpath + 'dkt_parcellation_' + measure + '.csv') # parcellated data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062c1afd",
   "metadata": {},
   "source": [
    "### 4. Regression\n",
    "\n",
    "This (optional) step regresses out the influence of other variables on the cortical thickness values, resulting in residual values that can be used in following steps.\n",
    "\n",
    "Inputs are:\n",
    "- the parcellated data (N x 62)\n",
    "- a glimfile including other variables (e.g., age for this example)\n",
    "- the mean thickness values computed above\n",
    "\n",
    "Outpus are:\n",
    "- a dataframe with the residuals of the dimensions N x 62\n",
    "\n",
    "___\n",
    "\n",
    "For high performance computing see:\n",
    "- regression.py\n",
    "- regression.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25973d1-6442-486b-987c-f4ba7b8f570d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read files\n",
    "glim = pd.read_csv(dpath + gfile, index_col='eid', dtype={'eid':str})\n",
    "parc = pd.read_csv(dpath + 'dkt_parcellation_' + measure + '.csv', index_col='eid', dtype={'eid':str})\n",
    "mean_measure = pd.read_csv(dpath + 'mean_' + measure + '.csv', index_col='eid', dtype={'eid':str})\n",
    "glim = glim[glim.index.isin(parc.index)]\n",
    "\n",
    "# merge glimfile and parcellated data\n",
    "glim = glim.join(mean_measure) # merge glim & mean thickness # modify to total_measure if using surface_area\n",
    "glim_parc = glim.join(parc) # join glim + mean thickness & parcellated data\n",
    "rois = parc.columns\n",
    "\n",
    "# regression\n",
    "if group in glim_parc.columns: # group-based regressions if applicable\n",
    "    for g in glim_parc[group].unique():\n",
    "        resid=[]\n",
    "        glim_parc_group = glim_parc[glim_parc[group]==g]\n",
    "        parc_group = glim_parc_group[rois]\n",
    "        covar1 = glim_parc_group[covars[0]]\n",
    "        covar2 = glim_parc_group[covars[1]]\n",
    "        for i in parc_group:\n",
    "            reg = sm.ols('parc_group.loc[:,i] ~ covar1 + covar2', data=glim_parc_group).fit()\n",
    "            residuals = reg.resid\n",
    "            resid.append(residuals)\n",
    "        \n",
    "        # merge groups\n",
    "        if g == glim_parc[group].unique()[0]:\n",
    "            res = pd.DataFrame(resid).T\n",
    "            res.columns = parc.columns\n",
    "        else:\n",
    "            tmp = pd.DataFrame(resid).T\n",
    "            tmp.columns = parc.columns\n",
    "            res = pd.concat([res,tmp], sort=True)\n",
    "else:\n",
    "    resid=[]\n",
    "    covar1 = glim_parc[covars[0]]\n",
    "    covar2 = glim_parc[covars[1]]\n",
    "    for i in parc:\n",
    "        reg = sm.ols('parc.loc[:,i] ~ covar1 + covar2', data=glim_parc).fit()\n",
    "        residuals = reg.resid\n",
    "        resid.append(residuals)\n",
    "    res = pd.DataFrame(resid).T\n",
    "    res.columns = parc.columns\n",
    "\n",
    "# write to csv\n",
    "glim.to_csv(dpath + 'glimfile.csv')\n",
    "glim_parc.to_csv(dpath + 'glimfile_parcellation.csv')\n",
    "res.to_csv(dpath + 'residuals.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1949922a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5. Structural Covariance \n",
    "\n",
    "This step performs pearson correlations between DKT regions for all participants, resulting in a structural covariance matrix of the full sample ([Alexander-Bloch et al., 2013a](https://doi.org/10.1038/nrn3465); [Alexander-Bloch et al., 2013b](https://doi.org/10.1523/JNEUROSCI.3554-12.2013); [Evans, 2013](https://doi.org/10.1016/j.neuroimage.2013.05.054)).\n",
    "\n",
    "Inputs are:\n",
    "- the residual data of the DKT regions\n",
    "\n",
    "Outputs are:\n",
    "- a structural covariance matrix of the full sample with the dimensions 62 x 62\n",
    "\n",
    "___\n",
    "\n",
    "For high performance computing see:\n",
    "- strucov.py\n",
    "- strucov.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ddc442",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read files\n",
    "#glim_parc = pd.read_csv(dpath + 'glimfile_parcellation.csv', index_col=[idvar], dtype={'eid':str})\n",
    "#res = pd.read_csv(dpath + 'residuals.csv', index_col=[idvar], dtype={'eid':str}) # read residuals\n",
    "\n",
    "# full sample (or group) correlation matrix\n",
    "if group in glim_parc.columns:\n",
    "    for g in glim_parc[group].unique():\n",
    "        res_group = res.loc[glim_parc[group]==g]\n",
    "        corrmtrix = res_group.corr(method='pearson')\n",
    "        corrmtrix.to_csv(dpath + 'corrmtrix_full_group_' + str(g) + '.csv') # write to csv\n",
    "else:\n",
    "    corrmtrix = res.corr(method='pearson')\n",
    "    corrmtrix.to_csv(dpath + 'corrmtrix_full.csv') # write to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea79fe36",
   "metadata": {},
   "source": [
    "### 6. Jackknife Bias Estimation Procedure \n",
    "\n",
    "This step calculates the contribution of each participant to the structural covariance matrix of the full sample ([Ajnakina et al., 2021](https://doi.org/10.1093/schbul/sbab035); [Das et al., 2018](https://doi.org/10.1001/jamapsychiatry.2018.0391)). Here, a structural covariance matrix is recalculated for each iteration of N-1 participants, meaning that the structural covariance between DKT regions is recalculated N times, by leaving each participant out of the calculation once. By then subtracting these recalculated matrices from the structural covariance matrix of the full sample, the contribution of each participant to the full-sample structural covariance is estimated. Absolute values are taken and the end result is a 3D matrix with the dimensions 62 x 62 x N.\n",
    "\n",
    "Inputs are:\n",
    "- the residual data of the DKT regions\n",
    "- the structural covariance matrix of the full sample\n",
    "\n",
    "Outputs are:\n",
    "- an absolute value 3D matrix of the dimensions 62 x 62 x N\n",
    "    - each of the N matrices represents the absolute contribution of each participant to the full-sample structural covariance matrix\n",
    "\n",
    "*NOTE.* For analyses with multiple groups (e.g., patient & control), jackknife should be performed on each group separately.\n",
    "___\n",
    "\n",
    "For high performance computing see:\n",
    "- jackknife.py\n",
    "- jackknife.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489641b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read files\n",
    "#glim_parc = pd.read_csv(dpath + 'glimfile_parcellation.csv', index_col=[idvar], dtype={'eid':str})\n",
    "#res = pd.read_csv(dpath + 'residuals.csv', index_col=[idvar], dtype={'eid':str})\n",
    "\n",
    "# jackknife correlation \n",
    "if group in glim_parc.columns:\n",
    "    for g in glim_parc[group].unique():\n",
    "        glim_parc_group = glim_parc[glim_parc[group]==g]\n",
    "        res_group = res.loc[glim_parc[group]==g]\n",
    "        corrmtrix = pd.read_csv(dpath + 'corrmtrix_full_group_' + str(g) + '.csv', index_col=[0])\n",
    "        jack = []\n",
    "        n = len(glim_parc_group[:]) # full sample\n",
    "        for i,row in res_group.iterrows(): # i = eid, so the loop goes over each row in the dataframe meaning each participant\n",
    "            LOO = res_group.drop(i) # and drops one row depending on i per loop (LOO = leave one out)\n",
    "            corrLOO = LOO.corr(method='pearson');\n",
    "            W = (n*corrmtrix)-((n-1)*corrLOO);\n",
    "            # Absolute\n",
    "            normW = abs(W);\n",
    "            jack.append(normW)\n",
    "        jk = np.array(jack)\n",
    "        np.save(dpath + 'jackknife_output_group_' + str(g) + '.npy', jk) # write out as numpy array\n",
    "else:\n",
    "    jack = []\n",
    "    n = len(glim_parc[:]) # full sample\n",
    "    corrmtrix = pd.read_csv(dpath + 'corrmtrix_full.csv', index_col=[0])\n",
    "    for i,row in res.iterrows(): # i = eid, so the loop goes over each row in the dataframe meaning each participant\n",
    "        LOO = res.drop(i) # and drops one row depending on i per loop (LOO = leave one out)\n",
    "        corrLOO = LOO.corr(method='pearson'); \n",
    "        W = (n*corrmtrix)-((n-1)*corrLOO);\n",
    "        # Absolute\n",
    "        normW = abs(W);\n",
    "        jack.append(normW)\n",
    "    jk = np.array(jack)\n",
    "    np.save(dpath + 'jackknife_output.npy', jk) # write out as numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c599a609",
   "metadata": {},
   "source": [
    "### 7. Graph Theory \n",
    "\n",
    "This step calculates the two graph measures of local strength and global efficiency by the means of the [Brain Connectivity Toolbox](http://www.brain-connectivity-toolbox.net/) in Python ([bctpy](https://pypi.org/project/bctpy/); [Rubinov & Sporns, 2010](https://doi.org/10.1016/j.neuroimage.2009.10.003)).\n",
    "\n",
    "Inputs are:\n",
    "- the 3D jackknife connectivity matrix \n",
    "- the glimfile\n",
    "\n",
    "Outputs are:\n",
    "- local strengths\n",
    "- global efficiency \n",
    "\n",
    "___\n",
    "\n",
    "For high performance computing see:\n",
    "- graph.py\n",
    "- graph.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609a8d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read files\n",
    "glim = pd.read_csv(dpath + 'glimfile.csv', index_col=[idvar], dtype={'eid':str})\n",
    "\n",
    "# compute graph measures\n",
    "if group in glim.columns:\n",
    "    for g in glim[group].unique():\n",
    "        glim_group = glim[glim[group]==g]\n",
    "        jk = np.load(dpath +'jackknife_output_group_' + str(g) + '.npy')\n",
    "        \n",
    "        # fixing that the diagonals are all 0 \n",
    "        for i in jk:\n",
    "            np.fill_diagonal(i, 0, wrap=True)\n",
    "        \n",
    "        # strengths\n",
    "        def strengths_und(jk):\n",
    "            return np.sum(jk, axis=1)\n",
    "        strengths = strengths_und(jk)\n",
    "        s = pd.DataFrame(data=strengths, index=glim_group.index.copy())\n",
    "        s.columns = ['Strength_'+ str(i) for i in range(1, s.shape[1] + 1)]\n",
    "        tmp = glim_group.join(s)\n",
    "        \n",
    "        # global efficiency\n",
    "        globeff = []\n",
    "        for i in jk:\n",
    "            bct.efficiency_wei(i)\n",
    "            globeff.append(bct.efficiency_wei(i))\n",
    "        e = pd.DataFrame(data=globeff, index=glim_group.index.copy())\n",
    "        e.columns = ['Global Efficiency']\n",
    "        tmp = tmp.join(e)\n",
    "    \n",
    "        # merge groups\n",
    "        if g == glim[group].unique()[0]:\n",
    "            data_conn = tmp\n",
    "        else:\n",
    "            data_conn = pd.concat([data_conn,tmp])\n",
    "    \n",
    "    # write to csv\n",
    "    data_conn.to_csv(dpath + 'glimfile_jackknife_output.csv')\n",
    "else:\n",
    "    jk = np.load(dpath + 'jackknife_output.npy')\n",
    "    \n",
    "    # fixing that the diagonals are all 0\n",
    "    for i in jk:\n",
    "        np.fill_diagonal(i, 0, wrap=True)\n",
    "    \n",
    "    # strengths\n",
    "    def strengths_und(jk):\n",
    "        return np.sum(jk, axis=1)\n",
    "    strengths = strengths_und(jk)\n",
    "    s = pd.DataFrame(data=strengths, index=glim.index.copy())\n",
    "    s.columns = ['Strength_'+str(i) for i in range(1, s.shape[1] + 1)]\n",
    "    data_conn = glim.join(s)\n",
    "    \n",
    "    # global efficiency\n",
    "    globeff = []\n",
    "    for i in jk:\n",
    "        bct.efficiency_wei(i)\n",
    "        globeff.append(bct.efficiency_wei(i))\n",
    "    e = pd.DataFrame(data=globeff, index=glim.index.copy())\n",
    "    e.columns = ['Global Efficiency']\n",
    "    data_conn = data_conn.join(e)\n",
    "    \n",
    "    # write to csv\n",
    "    data_conn.to_csv(dpath + 'glimfile_jackknife_output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a42536d-0058-43c5-b7e8-7f0d7729e9ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
