{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea54a450",
   "metadata": {},
   "source": [
    "## Jackknife Bias Estimation for Morphometric Connectivity\n",
    "\n",
    "This notebook describes the steps we used for calculating structural covariance and graph measures from CIVET-based cortical thickness:\n",
    "\n",
    "1. Install & Import Packages\n",
    "2. Read CIVET Files\n",
    "3. DKT Parcellation\n",
    "4. Covariate Regression\n",
    "5. Structural Covariance\n",
    "6. Jackknife Bias Estimation\n",
    "7. Graph Theory\n",
    "\n",
    "MATLAB-Code available [here](https://github.com/katielavigne/civetsurf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a10e38d",
   "metadata": {},
   "source": [
    "### 1. Install & Import Packages \n",
    "\n",
    "This step ensures all needed packages are either installed or, if already installed, imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f42e0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages needed for all steps of the analyses\n",
    "!pip install bctpy\n",
    "!pip install statsmodels\n",
    "!pip install pandas\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657b9e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import os, glob\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import bct\n",
    "import statsmodels.formula.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9652d30",
   "metadata": {},
   "source": [
    "### 2. Read Files\n",
    "\n",
    "This step reads [CIVET](https://www.bic.mni.mcgill.ca/ServicesSoftware/CIVET-2-1-0-Table-of-Contents)-based cortical surface text files from a specified directory. CIVET outputs text files consisting of  values for a given structural metric (cortical thickness, surface area, volume) for each vertex separately for each hemisphere. Using cortical thickness as an example with data from the [UK BioBank](https://www.ukbiobank.ac.uk/), this script loads the files for both hemispheres and merges them into a dataframe displaying all vertices. Further, mean cortical metric values are calculated for each participant by taking the mean of all vertices per participant, and a total metric value is calculated by summing the values of each participant. Depending on the cortical metric, one would use either the mean (thickness) or total (surface area) values for later steps.\n",
    "\n",
    "Inputs are:\n",
    "- cortical thickness files for each hemisphere of N participants\n",
    "\n",
    "Outputs are:\n",
    "- a dataframe with the dimensions N x 81,924 vertices\n",
    "- a dataframe with mean thickness values\n",
    "- a dataframe with total thickness values \n",
    "\n",
    "___\n",
    "\n",
    "For high performance computing, see:\n",
    "- readfiles.py\n",
    "- readfiles.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b01130b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define measures and paths for Step 2. \n",
    "measure = \"thickness\"\n",
    "civpath = \"/project/def-mlepage/ukbb/civet/\" + measure + \"/\"\n",
    "outdir = \"/scratch/katie/ukbb/data/\"\n",
    "dname = 'ukbb_' + measure + '_vertexdata.pkl'\n",
    "idvar = \"eid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb967a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find files & IDs\n",
    "Lfiles = glob.glob(civpath + '*left*')\n",
    "Lfiles.sort()\n",
    "Rfiles = glob.glob(civpath + '*right*')\n",
    "Rfiles.sort()\n",
    "subjIDs = [ids.split('/')[-1].split('_')[1] for ids in Lfiles]\n",
    "\n",
    "# Make dataframe & save as pickle\n",
    "Ldf = pd.concat((pd.read_csv(Lf, dtype=float, header=None).T for Lf in Lfiles))\n",
    "Rdf = pd.concat((pd.read_csv(Rf, dtype=float, header=None).T for Rf in Rfiles))\n",
    "df = pd.concat([Ldf,Rdf], axis=1)\n",
    "df.index = subjIDs\n",
    "df.index.names = [idvar]\n",
    "df.to_pickle(outdir + dname)\n",
    "\n",
    "# Calculate and save mean anatomical measure (mean thickness)\n",
    "mean_measure = pd.DataFrame(pd.to_numeric(df.mean(axis=1)), columns=[\"mean_\" + measure])\n",
    "mean_measure.to_csv(outdir + 'mean_' + measure + '.csv')\n",
    "\n",
    "# Calculate and save total anatomical measure (total thickness)\n",
    "tot_measure = pd.DataFrame(pd.to_numeric(df.sum(axis=1)), columns=[\"total_\" + measure])\n",
    "tot_measure.to_csv(outdir + 'total_' + measure + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fa51e5",
   "metadata": {},
   "source": [
    "### 3. DKT-Parcellation \n",
    "\n",
    "This step parcellates the data of the 81,924 vertices into 62 DKT-regions ([Klein & Tourville, 2012](https://doi.org/10.3389/fnins.2012.00171)).\n",
    "\n",
    "Inputs are:\n",
    "- the previously created dataframe with all 81,924 vertices \n",
    "- a DKT-file indicating the DKT regions (see the separate file \"DKT.csv\" in the repository; from [MATLAB code](https://github.com/katielavigne/civetsurf) and the [CIVET Manual](https://www.bic.mni.mcgill.ca/ServicesSoftware/CIVET-2-1-0-Table-of-Contents))\n",
    "- a DKT-file indicating which vertex belongs to which DKT-region (see the separate file \"CIVET_2.0_DKT.txt\" in the repository; from [MATLAB code](https://github.com/katielavigne/civetsurf) and the [CIVET Manual](https://www.bic.mni.mcgill.ca/ServicesSoftware/CIVET-2-1-0-Table-of-Contents))\n",
    "\n",
    "Outputs are:\n",
    "- a dataframe with the parcellated data of the dimensions N x 62 \n",
    "\n",
    "___\n",
    "\n",
    "For high performance computing see:\n",
    "- parcellation.py\n",
    "- parcellation.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355f482d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define measures and paths for Step 3. \n",
    "dpath = '/scratch/katie/ukbb/data/'\n",
    "ppath = '/project/def-mlepage/ukbb/civet/'\n",
    "dktvert_file = 'CIVET_2.0_DKT.txt'\n",
    "dktinfo_file = 'DKT.csv'\n",
    "roivar = \"roi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37040784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parcellation\n",
    "dktvert = pd.read_csv(ppath + dktvert_file, dtype=str, names= [roivar], header=None)\n",
    "dktinfo = pd.read_csv(ppath + dktinfo_file, dtype=str)\n",
    "\n",
    "parc = pd.DataFrame(index= df.index.copy())\n",
    "for r in range(len(dktinfo)):\n",
    "    roi = dktinfo.label_number[r]\n",
    "    abr = dktinfo.abbreviation[r]\n",
    "    means = pd.DataFrame(df.iloc[:,dktvert.index[dktvert.roi == roi]].mean(axis=1),columns=[abr], index= df.index.copy())\n",
    "    parc = pd.concat([parc,means], axis = 1)\n",
    "\n",
    "parc.to_csv(dpath + 'dkt_parcellation_' + measure + '.csv') # parcellated data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062c1afd",
   "metadata": {},
   "source": [
    "### 4. Regression\n",
    "\n",
    "This (optional) step regresses out the influence of other variables on the cortical thickness values, resulting in residual values that can be used in following steps.\n",
    "\n",
    "Inputs are:\n",
    "- the parcellated data (N x 62)\n",
    "- a glimfile including other variables (e.g., age for this example)\n",
    "- the mean thickness values computed above\n",
    "\n",
    "Outpus are:\n",
    "- a dataframe with the residuals of the dimensions N x 62\n",
    "\n",
    "___\n",
    "\n",
    "For high performance computing see:\n",
    "- regression.py\n",
    "- regression.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9a4670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define measures and paths for Step 4.\n",
    "gfile = 'ukbb_controls.csv'\n",
    "glim = pd.read_csv(os.path.join(dpath, gfile), index_col=[idvar]) # read glimfile \n",
    "glim = glim.join(mean_measure) # merge glim & mean thickness\n",
    "glim_parc = glim.join(parc) # #join glim + mean thickness & parcellated data\n",
    "covar1 = glim_parc.age_assess_t2\n",
    "covar2 = glim_parc.mean_thickness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c663f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression\n",
    "resid=[]\n",
    "for i in parc:\n",
    "    reg = sm.ols('parc.loc[:,i] ~ covar1 + covar2', data=glim_parc).fit()\n",
    "    residuals = reg.resid\n",
    "    resid.append(residuals)\n",
    "\n",
    "res = pd.DataFrame(resid).T\n",
    "res.columns=parc.columns\n",
    "res.to_csv('residuals.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1949922a",
   "metadata": {},
   "source": [
    "### 5. Structural Covariance \n",
    "\n",
    "This step performs pearson correlations between DKT regions for all participants, resulting in a structural covariance matrix of the full sample ([Alexander-Bloch et al., 2013a](https://doi.org/10.1038/nrn3465); [Alexander-Bloch et al., 2013b](https://doi.org/10.1523/JNEUROSCI.3554-12.2013); [Evans, 2013](https://doi.org/10.1016/j.neuroimage.2013.05.054)).\n",
    "\n",
    "Inputs are:\n",
    "- the residual data of the DKT regions\n",
    "\n",
    "Outputs are:\n",
    "- a structural covariance matrix of the full sample with the dimensions 62 x 62\n",
    "\n",
    "___\n",
    "\n",
    "For high performance computing see:\n",
    "- strucov.py\n",
    "- strucov.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ddc442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full sample correlation matrix\n",
    "corrmtrix = res.corr(method=\"pearson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea79fe36",
   "metadata": {},
   "source": [
    "### 6. Jackknife Bias Estimation Procedure \n",
    "\n",
    "This step calculates the contribution of each participant to the structural covariance matrix of the full sample ([Ajnakina et al., 2021](https://doi.org/10.1093/schbul/sbab035); [Das et al., 2018](https://doi.org/10.1001/jamapsychiatry.2018.0391)). Here, a structural covariance matrix is recalculated for each iteration of N-1 participants, meaning that the structural covariance between DKT regions is recalculated N times, by leaving each participant out of the calculation once. By then subtracting these recalculated matrices from the structural covariance matrix of the full sample, the contribution of each participant to the full-sample structural covariance is estimated. Absolute values are taken and the end result is a 3D matrix with the dimensions 62 x 62 x N.\n",
    "\n",
    "Inputs are:\n",
    "- the residual data of the DKT regions\n",
    "- the structural covariance matrix of the full sample\n",
    "\n",
    "Outputs are:\n",
    "- an absolute value 3D matrix of the dimensions 62 x 62 x N\n",
    "    - each of the N matrices represents the absolute contribution of each participant to the full-sample structural covariance matrix\n",
    "\n",
    "*NOTE.* For analyses with multiple groups (e.g., patient & control), jackknife should be performed on each group separately.\n",
    "___\n",
    "\n",
    "For high performance computing see:\n",
    "- jackknife.py\n",
    "- jackknife.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489641b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jackknife correlation \n",
    "jack = []\n",
    "n = len(glim_parc[:]) #full sample \n",
    "for i,row in res.iterrows(): #i = eid, so the loop goes over each row in the dataframe meaning each participant\n",
    "    LOO = res.drop(i) #and drops one row depending on i per loop \n",
    "    #print(LOO) #check if matrices have size of N-1 x 62 (sample -1 x DKT regions)\n",
    "    corrLOO = LOO.corr(method=\"pearson\"); \n",
    "    W = (n*corrmtrix)-((n-1)*corrLOO);\n",
    "\n",
    "    #% Absolute \n",
    "    normW = abs(W);\n",
    "    jack.append(normW)\n",
    "\n",
    "jk = np.array(jack)\n",
    "np.save('jackknife_output.npy', jk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c599a609",
   "metadata": {},
   "source": [
    "### 7. Graph Theory \n",
    "\n",
    "This step calculates the two graph measures of local strength and global efficiency by the means of the [Brain Connectivity Toolbox](http://www.brain-connectivity-toolbox.net/) in Python ([bctpy](https://pypi.org/project/bctpy/); [Rubinov & Sporns, 2010](https://doi.org/10.1016/j.neuroimage.2009.10.003)).\n",
    "\n",
    "Inputs are:\n",
    "- the 3D jackknife connectivity matrix \n",
    "- the glimfile\n",
    "\n",
    "Outputs are:\n",
    "- local strengths\n",
    "- global efficiency \n",
    "\n",
    "___\n",
    "\n",
    "For high performance computing see:\n",
    "- graph.py\n",
    "- graph.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609a8d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing that the diagonals are all 0 \n",
    "for i in jk:\n",
    "     np.fill_diagonal(i, 0, wrap=True)\n",
    "\n",
    "# Strengths\n",
    "def strengths_und(jk):\n",
    "    return np.sum(jk, axis=1) \n",
    "\n",
    "strengths = strengths_und(jk)\n",
    "s = pd.DataFrame(data = strengths, index = glim.index.copy())\n",
    "s.columns=[\"Strength_\"+str(i) for i in range(1, s.shape[1] + 1)]\n",
    "data_conn = glim.join(s) \n",
    "\n",
    "# Global Efficiency\n",
    "globeff= []\n",
    "for i in jk:\n",
    "    bct.efficiency_wei(i) \n",
    "    globeff.append(bct.efficiency_wei(i))\n",
    "    \n",
    "e = pd.DataFrame(data = globeff, index = glim.index.copy())\n",
    "e.columns=[\"Global Efficiency\"]\n",
    "data_conn = data_conn.join(e)  \n",
    "\n",
    "# Save dataframe\n",
    "data_conn.to_csv('ukbb_data_crystal_jackknife_graph_theory.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
